{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00bb197c",
      "metadata": {},
      "source": [
        "## Stage 1: Supervised Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b56d87e",
      "metadata": {},
      "source": [
        "#### 1. Install all packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0d46c084-5adb-4509-aa43-4294524d0677",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers trl peft bitsandbytes datasets\n",
        "!pip install -q rouge_score bert_score\n",
        "!pip install -q evaluate nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526500fa",
      "metadata": {},
      "source": [
        "#### 2. Import all modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "886ab934-7880-481e-a7f6-4bc80ba52bde",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83a829d3-9a32-41d4-af69-4cbc122dd7cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant.*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "28dddda2-ba01-46fc-bba9-eb81ade6f32c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    base_model_id = \"Qwen/Qwen2.5-3B\"\n",
        "    sft_model_id = \"./qwen-2.5-3b-sft-truthfulqa/sft\"\n",
        "    dpo_model_id = \"./qwen-2.5-3b-dpo-truthfulqa/dpo\"\n",
        "\n",
        "    dataset_id = \"truthfulqa/truthful_qa\"\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df5e0a5",
      "metadata": {},
      "source": [
        "#### 3. Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5e016fc7-c25e-49ed-b03a-5378afd8df7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_sample(sample):\n",
        "    question = sample.get(\"question\", \"\").strip()\n",
        "    answer = sample.get(\"best_answer\", \"\").strip()\n",
        "\n",
        "    formatted_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "    \n",
        "    return {\"text\": formatted_text}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "32aca67c-596d-4039-8206-b15cac0145ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "full_dataset = load_dataset(config.dataset_id, \"generation\")[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "43f9562f-06e1-4ba5-aa59-99ad1da4da8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fd309935-2e23-4091-9301-4c163018a55b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_dataset(element):\n",
        "    sample = format_sample(element)\n",
        "    outputs = tokenizer(\n",
        "        sample[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=512,\n",
        "        return_overflowing_tokens=False,\n",
        "        return_length=False,\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": outputs[\"input_ids\"],\n",
        "        \"attention_mask\": outputs[\"attention_mask\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d73032b2",
      "metadata": {},
      "source": [
        "#### 4. Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "62bfb894-3dce-47e3-81b3-928588b8c2af",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset = full_dataset.map(\n",
        "    tokenize_dataset,\n",
        "    batched=False,\n",
        "    remove_columns=full_dataset.column_names\n",
        ")\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "temp_split = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "\n",
        "dataset_split = {\n",
        "    \"train\": split_dataset[\"train\"],\n",
        "    \"validation\": temp_split[\"train\"],\n",
        "    \"test\": temp_split[\"test\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97372bcf",
      "metadata": {},
      "source": [
        "#### 5. Initialize QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3fff635e-d25d-4e67-8615-5c934cb5e168",
      "metadata": {},
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b732d3ed",
      "metadata": {},
      "source": [
        "#### 6. Download SLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "575830a4-7cd2-4582-9eea-54d89dd960b9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7266bdb9ac2e4d83803d4e6a727595ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e2cea19",
      "metadata": {},
      "source": [
        "#### 7. Initialize hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3f519b1d-6b28-40a0-bf74-16f5fb5229ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_training_args = SFTConfig(\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=100,\n",
        "    num_train_epochs=100,\n",
        "    learning_rate=3e-5,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        "    output_dir=config.sft_model_id,\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"best\",\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    load_best_model_at_end=True,\n",
        "    max_seq_length=2048,\n",
        "    dataset_num_proc=4,\n",
        "    packing=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af754288",
      "metadata": {},
      "source": [
        "#### 8. Intialize data collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bbc988d8-2635-4575-a427-2ab5b75b971c",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329085f3",
      "metadata": {},
      "source": [
        "#### 9. Prepare evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "aa64e223-0e66-4669-849c-4c99272bbe45",
      "metadata": {},
      "outputs": [],
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "exact_match_metric = evaluate.load(\"exact_match\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    # Handle tuple for predictions\n",
        "    predictions = predictions[0] if isinstance(predictions, tuple) else predictions\n",
        "\n",
        "    # Convert logits to predicted token IDs if needed\n",
        "    if predictions.ndim == 3:\n",
        "        predictions = predictions.argmax(-1)\n",
        "\n",
        "    # Convert tensors to lists\n",
        "    if hasattr(predictions, \"tolist\"):\n",
        "        predictions = predictions.tolist()\n",
        "    if hasattr(labels, \"tolist\"):\n",
        "        labels = labels.tolist()\n",
        "\n",
        "    # Replace -100 in labels with tokenizer.pad_token_id for decoding\n",
        "    labels = [\n",
        "        [token if token != -100 else tokenizer.pad_token_id for token in label_seq]\n",
        "        for label_seq in labels\n",
        "    ]\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Strip extra whitespace\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu_result[\"bleu\"],\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"],\n",
        "        \"bertscore_f1\": np.mean(bertscore_result[\"f1\"]),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2074c9",
      "metadata": {},
      "source": [
        "#### 10. Fine tune the SLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6cd61670-ec5d-4817-b808-92b5b03a0af9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_training_args,\n",
        "    train_dataset=dataset_split[\"train\"],\n",
        "    eval_dataset=dataset_split[\"test\"],\n",
        "    peft_config=lora_config,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "39fdf6bc-a8d6-4d48-9f34-cfc86a79e42c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='360' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 360/1200 06:15 < 14:41, 0.95 it/s, Epoch 30/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Bertscore F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.443800</td>\n",
              "      <td>2.448460</td>\n",
              "      <td>0.176618</td>\n",
              "      <td>0.484373</td>\n",
              "      <td>0.445929</td>\n",
              "      <td>0.863016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.406400</td>\n",
              "      <td>2.431044</td>\n",
              "      <td>0.171053</td>\n",
              "      <td>0.483627</td>\n",
              "      <td>0.443920</td>\n",
              "      <td>0.864467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.168700</td>\n",
              "      <td>2.400669</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.487869</td>\n",
              "      <td>0.448784</td>\n",
              "      <td>0.866754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.472600</td>\n",
              "      <td>2.352984</td>\n",
              "      <td>0.174265</td>\n",
              "      <td>0.491813</td>\n",
              "      <td>0.453535</td>\n",
              "      <td>0.867944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.314500</td>\n",
              "      <td>2.284949</td>\n",
              "      <td>0.205219</td>\n",
              "      <td>0.509459</td>\n",
              "      <td>0.468960</td>\n",
              "      <td>0.867765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.303000</td>\n",
              "      <td>2.200385</td>\n",
              "      <td>0.218563</td>\n",
              "      <td>0.526093</td>\n",
              "      <td>0.482670</td>\n",
              "      <td>0.871496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.258200</td>\n",
              "      <td>2.119777</td>\n",
              "      <td>0.241026</td>\n",
              "      <td>0.560808</td>\n",
              "      <td>0.509079</td>\n",
              "      <td>0.877684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.153500</td>\n",
              "      <td>2.035911</td>\n",
              "      <td>0.254487</td>\n",
              "      <td>0.567482</td>\n",
              "      <td>0.515033</td>\n",
              "      <td>0.879413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.767900</td>\n",
              "      <td>1.942786</td>\n",
              "      <td>0.256733</td>\n",
              "      <td>0.567723</td>\n",
              "      <td>0.517811</td>\n",
              "      <td>0.880178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.833900</td>\n",
              "      <td>1.860202</td>\n",
              "      <td>0.261583</td>\n",
              "      <td>0.573882</td>\n",
              "      <td>0.518836</td>\n",
              "      <td>0.880767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.708100</td>\n",
              "      <td>1.804735</td>\n",
              "      <td>0.265673</td>\n",
              "      <td>0.575469</td>\n",
              "      <td>0.526928</td>\n",
              "      <td>0.881475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.916100</td>\n",
              "      <td>1.763784</td>\n",
              "      <td>0.272021</td>\n",
              "      <td>0.574302</td>\n",
              "      <td>0.533344</td>\n",
              "      <td>0.880706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.721200</td>\n",
              "      <td>1.728448</td>\n",
              "      <td>0.266850</td>\n",
              "      <td>0.570214</td>\n",
              "      <td>0.531431</td>\n",
              "      <td>0.879702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.681400</td>\n",
              "      <td>1.697585</td>\n",
              "      <td>0.270223</td>\n",
              "      <td>0.576196</td>\n",
              "      <td>0.539204</td>\n",
              "      <td>0.880943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.612700</td>\n",
              "      <td>1.669081</td>\n",
              "      <td>0.270538</td>\n",
              "      <td>0.585607</td>\n",
              "      <td>0.549617</td>\n",
              "      <td>0.881322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.661100</td>\n",
              "      <td>1.645847</td>\n",
              "      <td>0.273981</td>\n",
              "      <td>0.593369</td>\n",
              "      <td>0.557626</td>\n",
              "      <td>0.882417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.480000</td>\n",
              "      <td>1.625383</td>\n",
              "      <td>0.276728</td>\n",
              "      <td>0.599302</td>\n",
              "      <td>0.565387</td>\n",
              "      <td>0.883191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.720200</td>\n",
              "      <td>1.610362</td>\n",
              "      <td>0.276959</td>\n",
              "      <td>0.603232</td>\n",
              "      <td>0.565673</td>\n",
              "      <td>0.883599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.622400</td>\n",
              "      <td>1.599045</td>\n",
              "      <td>0.283146</td>\n",
              "      <td>0.608603</td>\n",
              "      <td>0.571480</td>\n",
              "      <td>0.883407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.787000</td>\n",
              "      <td>1.590628</td>\n",
              "      <td>0.284372</td>\n",
              "      <td>0.610147</td>\n",
              "      <td>0.573102</td>\n",
              "      <td>0.883824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.537200</td>\n",
              "      <td>1.582204</td>\n",
              "      <td>0.291139</td>\n",
              "      <td>0.612987</td>\n",
              "      <td>0.576675</td>\n",
              "      <td>0.884660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.690400</td>\n",
              "      <td>1.575088</td>\n",
              "      <td>0.289867</td>\n",
              "      <td>0.615694</td>\n",
              "      <td>0.579472</td>\n",
              "      <td>0.884883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.449400</td>\n",
              "      <td>1.570774</td>\n",
              "      <td>0.288168</td>\n",
              "      <td>0.608981</td>\n",
              "      <td>0.575273</td>\n",
              "      <td>0.885443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.631800</td>\n",
              "      <td>1.565140</td>\n",
              "      <td>0.291592</td>\n",
              "      <td>0.610982</td>\n",
              "      <td>0.579204</td>\n",
              "      <td>0.886294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.495900</td>\n",
              "      <td>1.561561</td>\n",
              "      <td>0.291873</td>\n",
              "      <td>0.610039</td>\n",
              "      <td>0.579274</td>\n",
              "      <td>0.885283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.489200</td>\n",
              "      <td>1.559102</td>\n",
              "      <td>0.295016</td>\n",
              "      <td>0.614388</td>\n",
              "      <td>0.582051</td>\n",
              "      <td>0.884968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.439600</td>\n",
              "      <td>1.554546</td>\n",
              "      <td>0.294391</td>\n",
              "      <td>0.613024</td>\n",
              "      <td>0.582446</td>\n",
              "      <td>0.884849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.443000</td>\n",
              "      <td>1.559123</td>\n",
              "      <td>0.294867</td>\n",
              "      <td>0.610043</td>\n",
              "      <td>0.578286</td>\n",
              "      <td>0.884724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.479500</td>\n",
              "      <td>1.557718</td>\n",
              "      <td>0.296272</td>\n",
              "      <td>0.612791</td>\n",
              "      <td>0.579100</td>\n",
              "      <td>0.885056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.486000</td>\n",
              "      <td>1.555143</td>\n",
              "      <td>0.295017</td>\n",
              "      <td>0.607993</td>\n",
              "      <td>0.576795</td>\n",
              "      <td>0.884386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "train_history = sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3db95195-14f6-422a-ac1c-0320884f76be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=360, training_loss=1.8511671821276348, metrics={'train_runtime': 377.314, 'train_samples_per_second': 194.798, 'train_steps_per_second': 3.18, 'total_flos': 1.831223777181696e+16, 'train_loss': 1.8511671821276348})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "734a66a6-be3a-4a53-93f7-c2f43bda6675",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_history= sft_trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "82a15eaa-1e00-49d9-a055-ff9f754266de",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.5551425218582153,\n",
              " 'eval_bleu': 0.29501719344848476,\n",
              " 'eval_rouge1': 0.6079930559773326,\n",
              " 'eval_rougeL': 0.5767949236020923,\n",
              " 'eval_bertscore_f1': 0.8843864958460738,\n",
              " 'eval_runtime': 1.5284,\n",
              " 'eval_samples_per_second': 26.825,\n",
              " 'eval_steps_per_second': 3.926}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5c77e7f6-f2ff-4249-85e6-41a1ecbe354a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/teamspace/studios/this_studio'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "567a5629-40cd-4158-acc9-99e72e9238c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zip_all_files(output_filename='stage-1.zip', directory='qwen-2.5-3b-sft-truthfulqa'):\n",
        "    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for foldername, subfolders, filenames in os.walk(directory):\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(foldername, filename)\n",
        "                # Skip hidden files and system files if desired\n",
        "                if not filename.startswith('.') and '__pycache__' not in file_path:\n",
        "                    zipf.write(file_path, os.path.relpath(file_path, directory))\n",
        "\n",
        "zip_all_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a2a2ff-7e71-4112-9d8f-9e0f0ff292c6",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}